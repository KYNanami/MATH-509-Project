{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7915620a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7915620a",
    "outputId": "0d1a2ab1-7587-426b-e4a6-56c7fff44ac9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dbc0fa0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dbc0fa0",
    "outputId": "6611782b-a88e-46e3-8014-ae213258ae90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    IDLink                                              Title  \\\n",
      "0  99248.0   Obama Lays Wreath at Arlington National Cemetery   \n",
      "1  10423.0        A Look at the Health of the Chinese Economy   \n",
      "2  18828.0   Nouriel Roubini: Global Economy Not Back to 2008   \n",
      "3  27788.0                          Finland GDP Expands In Q4   \n",
      "4  27789.0  Tourism, govt spending buoys Thai economy in J...   \n",
      "\n",
      "                                            Headline  \\\n",
      "0  Obama Lays Wreath at Arlington National Cemete...   \n",
      "1  Tim Haywood, investment director business-unit...   \n",
      "2  Nouriel Roubini, NYU professor and chairman at...   \n",
      "3  Finland's economy expanded marginally in the t...   \n",
      "4  Tourism and public spending continued to boost...   \n",
      "\n",
      "                                     Source    Topic          PublishDate  \\\n",
      "0                                 USA TODAY    obama  2002-04-02 00:00:00   \n",
      "1                                 Bloomberg  economy  2008-09-20 00:00:00   \n",
      "2                                 Bloomberg  economy  2012-01-28 00:00:00   \n",
      "3                                  RTT News  economy  2015-03-01 00:06:00   \n",
      "4  The Nation - Thailand&#39;s English news  economy  2015-03-01 00:11:00   \n",
      "\n",
      "   SentimentTitle  SentimentHeadline  Facebook  GooglePlus  LinkedIn  \n",
      "0        0.000000          -0.053300        -1          -1        -1  \n",
      "1        0.208333          -0.156386        -1          -1        -1  \n",
      "2       -0.425210           0.139754        -1          -1        -1  \n",
      "3        0.000000           0.026064        -1          -1        -1  \n",
      "4        0.000000           0.141084        -1          -1        -1  \n"
     ]
    }
   ],
   "source": [
    "# Load the CSV data into a DataFrame\n",
    "df = pd.read_csv(\"/Users/kynanami/Documents/Winter 2024/MATH 509/dataset/Data/News_Final.csv\", header=0)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify it's loaded correctly\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7df53c48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7df53c48",
    "outputId": "a855988e-aa41-42c0-b6c9-b1417778f3d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline     15\n",
      "Source      279\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(92945, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Handel missing data\n",
    "# Check for missing values in each column\n",
    "missing_data = df.isnull().sum()\n",
    "\n",
    "# Print the number of missing values in each column\n",
    "print(missing_data[missing_data > 0])\n",
    "\n",
    "#clean data\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56057f65",
   "metadata": {
    "id": "56057f65"
   },
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "850c939a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "850c939a",
    "outputId": "d9dabf5d-9021-4840-bb2a-5ed98f264375"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86480, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing duplicates from the \"Headline\" column.\n",
    "df = df.drop_duplicates(subset=['Headline'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3ff65e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3ff65e9",
    "outputId": "f9e95716-ceb5-47fd-e4aa-bbd07c9b85d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86480, 12)\n"
     ]
    }
   ],
   "source": [
    "# Removing purely numeric values from the \"Headline\" column.\n",
    "df = df[~df['Headline'].str.isnumeric()]\n",
    "\n",
    "# Pattern to match the specific numeric formats in headlines\n",
    "pattern = r'\\d+\\.[A-Z]{2,3}&gt;|\\b\\d+\\.\\d+\\b|\\d+am|\\d+pm|\\d+s|\\d{2,}'\n",
    "\n",
    "\n",
    "# Using str.replace to remove the matched patterns with an empty string, but only modify the 'Headline' column\n",
    "df['Headline'] = df['Headline'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Save the filtered DataFrame to a CSV file\n",
    "df.to_csv('no_numeric_headlines.csv', index=False)\n",
    "\n",
    "# Display the shape of the DataFrame to confirm no columns were removed\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d7ca681",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d7ca681",
    "outputId": "f15528e4-a787-4b15-fd36-1cba7db2cf94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Headline  \\\n",
      "0      Obama Lays Wreath at Arlington National Cemete...   \n",
      "1      Tim Haywood, investment director business-unit...   \n",
      "2      Nouriel Roubini, NYU professor and chairman at...   \n",
      "3      Finland's economy expanded marginally in the t...   \n",
      "4      Tourism and public spending continued to boost...   \n",
      "...                                                  ...   \n",
      "93234  The June employment report is viewed as a cruc...   \n",
      "93235  In addition, establish stimulating economic po...   \n",
      "93236  The Palestinian government spends nearly $ mil...   \n",
      "93237  Palestine Youth Orchestra prepares for first U...   \n",
      "93238  Goldstein, the proprietor of the TG Travel Gro...   \n",
      "\n",
      "                                          Clean_Headline  \n",
      "0      Obama Lays Wreath at Arlington National Cemete...  \n",
      "1      Tim Haywood, investment director business-unit...  \n",
      "2      Nouriel Roubini, NYU professor and chairman at...  \n",
      "3      Finland's economy expanded marginally in the t...  \n",
      "4      Tourism and public spending continued to boost...  \n",
      "...                                                  ...  \n",
      "93234  The June employment report is viewed as a cruc...  \n",
      "93235  In addition, establish stimulating economic po...  \n",
      "93236  The Palestinian government spends nearly $ mil...  \n",
      "93237  Palestine Youth Orchestra prepares for first U...  \n",
      "93238  Goldstein, the proprietor of the TG Travel Gro...  \n",
      "\n",
      "[86480 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "# Function to remove accents\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "# Apply the function to the 'Headline' column\n",
    "df['Clean_Headline'] = df['Headline'].apply(remove_accents)\n",
    "\n",
    "# Display the original and cleaned headlines\n",
    "print(df[['Headline', 'Clean_Headline']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73011f6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73011f6c",
    "outputId": "8427fb42-a386-4692-f49c-1881823d770c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Clean_Headline  \\\n",
      "0      Obama Lays Wreath at Arlington National Cemete...   \n",
      "1      Tim Haywood, investment director business-unit...   \n",
      "2      Nouriel Roubini, NYU professor and chairman at...   \n",
      "3      Finland's economy expanded marginally in the t...   \n",
      "4      Tourism and public spending continued to boost...   \n",
      "...                                                  ...   \n",
      "93234  The June employment report is viewed as a cruc...   \n",
      "93235  In addition, establish stimulating economic po...   \n",
      "93236  The Palestinian government spends nearly $ mil...   \n",
      "93237  Palestine Youth Orchestra prepares for first U...   \n",
      "93238  Goldstein, the proprietor of the TG Travel Gro...   \n",
      "\n",
      "                                        Cleaned_Headline  \n",
      "0      Lays Wreath at National President has laid a w...  \n",
      "1      investment director head for fixed income at d...  \n",
      "2      professor and chairman at Global explains why ...  \n",
      "3      economy expanded marginally in the three month...  \n",
      "4      Tourism and public spending continued to boost...  \n",
      "...                                                  ...  \n",
      "93234  The employment report is viewed as a crucial g...  \n",
      "93235  In establish stimulating economic We have to d...  \n",
      "93236  The government spends nearly $ million annuall...  \n",
      "93237  Palestine Youth Orchestra prepares for first t...  \n",
      "93238  the proprietor of the Travel Group custom tour...  \n",
      "\n",
      "[86480 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Initialize the spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Function to remove unrecognized words\n",
    "def remove_typos(text):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Check each word against the spell checker\n",
    "    valid_words = [word for word in words if word in spell or len(spell.unknown([word])) == 0]\n",
    "    # Join and return the valid words as a new string\n",
    "    return ' '.join(valid_words)\n",
    "\n",
    "# Apply the function to the 'Headline' column\n",
    "df['Cleaned_Headline'] = df['Clean_Headline'].apply(remove_typos)\n",
    "\n",
    "# Display the DataFrame to verify changes\n",
    "print(df[['Clean_Headline', 'Cleaned_Headline']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f8784fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f8784fe",
    "outputId": "5f16c684-82cf-4318-b581-e063a445cb38"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "\n",
    "word_list = set(words.words())\n",
    "\n",
    "def remove_non_dictionary_words(text):\n",
    "    return ' '.join(word for word in text.split() if word in word_list)\n",
    "\n",
    "# Assuming 'text' is a string containing your text data\n",
    "df['Cleaned_Headline'] = df['Cleaned_Headline'].apply(remove_non_dictionary_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e15d4b46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e15d4b46",
    "outputId": "ae75c881-fc95-4682-ec79-b84b64b42c6a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebff5110",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebff5110",
    "outputId": "b94d4025-90c7-4aa8-d6c6-c51a22997952"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [en:0.9999992118190077]\n",
       "1        [en:0.9999979793666663]\n",
       "2        [en:0.9999974964674774]\n",
       "3        [en:0.9999989029837117]\n",
       "4        [en:0.9999965625426681]\n",
       "                  ...           \n",
       "93234    [en:0.9999974227318642]\n",
       "93235    [en:0.9999951959686906]\n",
       "93236    [en:0.9999977005383116]\n",
       "93237    [en:0.9999970397597799]\n",
       "93238    [en:0.9999985071438429]\n",
       "Name: Cleaned_Headline, Length: 86480, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from langdetect import detect_langs\n",
    "from langdetect import detect_langs, LangDetectException\n",
    "\n",
    "# Detecting the language of each headline\n",
    "def detect_language_safe(text):\n",
    "    try:\n",
    "        return detect_langs(text)\n",
    "    except LangDetectException:\n",
    "        return ['unknown']\n",
    "\n",
    "sample_headlines = df['Cleaned_Headline'].astype(str)\n",
    "detected_languages = sample_headlines.apply(lambda x: detect_language_safe(x))\n",
    "detected_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9f56d56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9f56d56",
    "outputId": "1c0fd1ec-96ba-4119-a165-ebc991173b31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of headlines: 86480\n",
      "Number of headlines with only English Language headlines: 85839\n"
     ]
    }
   ],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        languages = detect_langs(text)\n",
    "        return languages[0].lang\n",
    "    except LangDetectException:\n",
    "        return None\n",
    "\n",
    "# Detect language in batches\n",
    "batch_size = 1000\n",
    "languages = []\n",
    "\n",
    "for i in range(0, df.shape[0], batch_size):\n",
    "    batch = df['Cleaned_Headline'][i:i+batch_size].astype(str)\n",
    "    batch_languages = batch.apply(detect_language)\n",
    "    languages.extend(batch_languages)\n",
    "\n",
    "# Adding detected languages as a new column to the DataFrame\n",
    "df['Language'] = languages\n",
    "\n",
    "# Filtering out non-English headlines\n",
    "df_filtered = df[df['Language'] == 'en']\n",
    "\n",
    "\n",
    "print(f\"Original number of headlines: {df.shape[0]}\")\n",
    "print(f\"Number of headlines with only English Language headlines: {df_filtered.shape[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f5baeb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "9f5baeb8",
    "outputId": "e6dca07d-4224-404f-9988-a4431f04aca7"
   },
   "outputs": [],
   "source": [
    "# Save the filtered DataFrame to a CSV file\n",
    "df_filtered.to_csv('filtered_headlines.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3136b7fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3136b7fb",
    "outputId": "0458e27c-a2a9-421f-8144-ace3fef2b666"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22cc2827",
   "metadata": {
    "id": "22cc2827"
   },
   "outputs": [],
   "source": [
    "#Preprocess the Text Data\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s\\+\\¬\\†]', ' ', text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(token) for token in lemmatized_text]\n",
    "\n",
    "    # Re-join tokens into a string\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b9a43d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b9a43d7",
    "outputId": "c769b278-233a-4441-b1e8-037bcc3fc64e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Headline  \\\n",
      "0      Obama Lays Wreath at Arlington National Cemete...   \n",
      "1      Tim Haywood, investment director business-unit...   \n",
      "2      Nouriel Roubini, NYU professor and chairman at...   \n",
      "3      Finland's economy expanded marginally in the t...   \n",
      "4      Tourism and public spending continued to boost...   \n",
      "...                                                  ...   \n",
      "93234  The June employment report is viewed as a cruc...   \n",
      "93235  In addition, establish stimulating economic po...   \n",
      "93236  The Palestinian government spends nearly $ mil...   \n",
      "93237  Palestine Youth Orchestra prepares for first U...   \n",
      "93238  Goldstein, the proprietor of the TG Travel Gro...   \n",
      "\n",
      "                                      Processed_Headline  \n",
      "0                                      laid wreath honor  \n",
      "1         invest director head fix incom beig book state  \n",
      "2                 professor chairman global economi face  \n",
      "3      economi expand margin three end previous preli...  \n",
      "4      public spend continu boost economi light contr...  \n",
      "...                                                  ...  \n",
      "93234  employ report crucial gaug health economi last...  \n",
      "93235                    establish econom budget premier  \n",
      "93236        govern near million annual accord testimoni  \n",
      "93237                                first tour give six  \n",
      "93238                proprietor custom tour among affect  \n",
      "\n",
      "[85839 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/8xzncctn7wvcmd0xlnrxhl580000gn/T/ipykernel_8291/1863648691.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['Processed_Headline'] = df_filtered['Cleaned_Headline'].apply(preprocess_text)\n"
     ]
    }
   ],
   "source": [
    "# Apply the preprocessing function to each headline\n",
    "df_filtered['Processed_Headline'] = df_filtered['Cleaned_Headline'].apply(preprocess_text)\n",
    "\n",
    "# Display the processed headlines\n",
    "print(df_filtered[['Headline', 'Processed_Headline']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ecff40",
   "metadata": {
    "id": "51ecff40"
   },
   "source": [
    "### Document-Term Matrix (DTM): Transform the preprocessed headlines into a DTM, which quantifies the headlines by the occurrence of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "affbaba9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "affbaba9",
    "outputId": "9e2f5014-2595-4027-e6e0-4a94998f9bc9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "540cb937",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "540cb937",
    "outputId": "85007e62-8415-451b-a67e-8d987fc54a85"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aback</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abhor</th>\n",
       "      <th>abid</th>\n",
       "      <th>abil</th>\n",
       "      <th>abject</th>\n",
       "      <th>abl</th>\n",
       "      <th>ablaz</th>\n",
       "      <th>...</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zero</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zing</th>\n",
       "      <th>zip</th>\n",
       "      <th>zloti</th>\n",
       "      <th>zombi</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85834</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85835</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85836</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85837</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85838</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85839 rows × 9606 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aback  abandon  abat  abbey  abhor  abid  abil  abject  abl  ablaz  \\\n",
       "0          0        0     0      0      0     0     0       0    0      0   \n",
       "1          0        0     0      0      0     0     0       0    0      0   \n",
       "2          0        0     0      0      0     0     0       0    0      0   \n",
       "3          0        0     0      0      0     0     0       0    0      0   \n",
       "4          0        0     0      0      0     0     0       0    0      0   \n",
       "...      ...      ...   ...    ...    ...   ...   ...     ...  ...    ...   \n",
       "85834      0        0     0      0      0     0     0       0    0      0   \n",
       "85835      0        0     0      0      0     0     0       0    0      0   \n",
       "85836      0        0     0      0      0     0     0       0    0      0   \n",
       "85837      0        0     0      0      0     0     0       0    0      0   \n",
       "85838      0        0     0      0      0     0     0       0    0      0   \n",
       "\n",
       "       ...  zeal  zero  zigzag  zinc  zing  zip  zloti  zombi  zone  zoo  \n",
       "0      ...     0     0       0     0     0    0      0      0     0    0  \n",
       "1      ...     0     0       0     0     0    0      0      0     0    0  \n",
       "2      ...     0     0       0     0     0    0      0      0     0    0  \n",
       "3      ...     0     0       0     0     0    0      0      0     0    0  \n",
       "4      ...     0     0       0     0     0    0      0      0     0    0  \n",
       "...    ...   ...   ...     ...   ...   ...  ...    ...    ...   ...  ...  \n",
       "85834  ...     0     0       0     0     0    0      0      0     0    0  \n",
       "85835  ...     0     0       0     0     0    0      0      0     0    0  \n",
       "85836  ...     0     0       0     0     0    0      0      0     0    0  \n",
       "85837  ...     0     0       0     0     0    0      0      0     0    0  \n",
       "85838  ...     0     0       0     0     0    0      0      0     0    0  \n",
       "\n",
       "[85839 rows x 9606 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "#\n",
    "dtm = vectorizer.fit_transform(df_filtered['Processed_Headline'])\n",
    "\n",
    "\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "# dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f63517a1-7057-46ff-b198-de7d22479d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=4)\n",
    "ldamodel = lda.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4d5a8f1-ba74-4163-a1fc-7ded2faba2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "economi econom said growth global percent govern quarter year grow\n",
      "Topic 1:\n",
      "first presid visit administr meet secur state peopl last time\n",
      "Topic 2:\n",
      "new compani today would work said use one make get\n",
      "Topic 3:\n",
      "economi econom state said presidenti two one would market last\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(ldamodel, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a870559b-393d-43fa-a07d-84373ea9844b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85839"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_dist = ldamodel.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "360f3df9-d63f-4dc0-a645-8d482f543578",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_topics = [None]*len(topic_dist)\n",
    "for i in range(0, len(topic_dist)-1):\n",
    "    classified_topics[i] = pd.Series(topic_dist[i]).idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac2b46bf-37fa-495c-beab-b466487d6ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23290 19686 25139 17723\n"
     ]
    }
   ],
   "source": [
    "print(classified_topics.count(0), classified_topics.count(1), classified_topics.count(2), classified_topics.count(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3633289",
   "metadata": {
    "id": "d3633289"
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da8c4705",
   "metadata": {
    "id": "da8c4705"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AV3D2LLpuX6R",
   "metadata": {
    "id": "AV3D2LLpuX6R"
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "dtm_sparse = sparse.csr_matrix(dtm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962cc84c",
   "metadata": {
    "id": "962cc84c"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "dtm_scaled = scaler.fit_transform(dtm_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "tDDNBrC7xbTa",
   "metadata": {
    "id": "tDDNBrC7xbTa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "index = np.arange(85919)\n",
    "batches = np.split(np.arange(90000), 9)\n",
    "batches[-1] = np.arange(80000, 85907)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41e5cd76",
   "metadata": {
    "id": "41e5cd76",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Initialize IncrementalPCA with default parameters\n",
    "ipca = IncrementalPCA()\n",
    "\n",
    "# Assuming you have a batches function defined properly\n",
    "for batch in batches:\n",
    "  dtm_batch = dtm_df.iloc[batch]\n",
    "  dtm_batch = preprocessing.scale(dtm_batch)\n",
    "  ipca.partial_fit(dtm_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "imWg6fKr4H2J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imWg6fKr4H2J",
    "outputId": "10117de2-908f-49a8-ff17-bee6cbd501f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9605"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_var = ipca.explained_variance_ratio_.cumsum()\n",
    "len(exp_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c256c2b6-4ca9-4f9b-a1e0-c9cbbd76bcf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exp_var' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mexp_var\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exp_var' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(exp_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b6c7375-f38e-4e1e-ad98-4c784ca6b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "pc_reduced = ipca.components_[:,:np.argmax(exp_var>threshold)]\n",
    "pc_reduced.shape\n",
    "np.savetxt(\"principal_components.csv\", pc_reduced, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103ce71",
   "metadata": {
    "id": "7103ce71"
   },
   "outputs": [],
   "source": [
    "# #Perform PCA directly\n",
    "\n",
    "# pca = PCA().fit(dtm_df)\n",
    "# cumulative_variance_ratio = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# # Find the # of components that account for a 95% cumulative variance\n",
    "# n_components = len(cumulative_variance_ratio[cumulative_variance_ratio <= 0.95]) + 1\n",
    "\n",
    "# n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29881b43",
   "metadata": {
    "id": "29881b43"
   },
   "outputs": [],
   "source": [
    "# # Initialize PCA, choose the number of components\n",
    "# pca = PCA(n_components=#)\n",
    "\n",
    "# # Fit PCA on the DTM or standardized DTM\n",
    "# dtm_pca = pca.fit_transform(dtm_df)\n",
    "\n",
    "# dtm_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b20f3",
   "metadata": {
    "id": "963b20f3"
   },
   "outputs": [],
   "source": [
    "# # Randomized PCA\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=100, svd_solver='randomized')\n",
    "\n",
    "# dtm_pca = pca.fit(dtm_df)\n",
    "\n",
    "# dtm_pca"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
