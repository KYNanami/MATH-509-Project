{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7915620a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7915620a",
    "outputId": "0d1a2ab1-7587-426b-e4a6-56c7fff44ac9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3dbc0fa0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dbc0fa0",
    "outputId": "6611782b-a88e-46e3-8014-ae213258ae90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    IDLink                                              Title  \\\n",
      "0  99248.0   Obama Lays Wreath at Arlington National Cemetery   \n",
      "1  10423.0        A Look at the Health of the Chinese Economy   \n",
      "2  18828.0   Nouriel Roubini: Global Economy Not Back to 2008   \n",
      "3  27788.0                          Finland GDP Expands In Q4   \n",
      "4  27789.0  Tourism, govt spending buoys Thai economy in J...   \n",
      "\n",
      "                                            Headline  \\\n",
      "0  Obama Lays Wreath at Arlington National Cemete...   \n",
      "1  Tim Haywood, investment director business-unit...   \n",
      "2  Nouriel Roubini, NYU professor and chairman at...   \n",
      "3  Finland's economy expanded marginally in the t...   \n",
      "4  Tourism and public spending continued to boost...   \n",
      "\n",
      "                                     Source    Topic          PublishDate  \\\n",
      "0                                 USA TODAY    obama  2002-04-02 00:00:00   \n",
      "1                                 Bloomberg  economy  2008-09-20 00:00:00   \n",
      "2                                 Bloomberg  economy  2012-01-28 00:00:00   \n",
      "3                                  RTT News  economy  2015-03-01 00:06:00   \n",
      "4  The Nation - Thailand&#39;s English news  economy  2015-03-01 00:11:00   \n",
      "\n",
      "   SentimentTitle  SentimentHeadline  Facebook  GooglePlus  LinkedIn  \n",
      "0        0.000000          -0.053300        -1          -1        -1  \n",
      "1        0.208333          -0.156386        -1          -1        -1  \n",
      "2       -0.425210           0.139754        -1          -1        -1  \n",
      "3        0.000000           0.026064        -1          -1        -1  \n",
      "4        0.000000           0.141084        -1          -1        -1  \n"
     ]
    }
   ],
   "source": [
    "# Load the CSV data into a DataFrame\n",
    "df = pd.read_csv(\"/Users/kynanami/Documents/Winter 2024/MATH 509/dataset/Data/News_Final.csv\", header=0)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify it's loaded correctly\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7df53c48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7df53c48",
    "outputId": "a855988e-aa41-42c0-b6c9-b1417778f3d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline     15\n",
      "Source      279\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(92945, 11)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Handel missing data\n",
    "# Check for missing values in each column\n",
    "missing_data = df.isnull().sum()\n",
    "\n",
    "# Print the number of missing values in each column\n",
    "print(missing_data[missing_data > 0])\n",
    "\n",
    "#clean data\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "56057f65",
   "metadata": {
    "id": "56057f65"
   },
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "850c939a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "850c939a",
    "outputId": "d9dabf5d-9021-4840-bb2a-5ed98f264375"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86553, 11)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing duplicates from the \"Headline\" column.\n",
    "df = df.drop_duplicates(subset=['Headline'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e3ff65e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3ff65e9",
    "outputId": "f9e95716-ceb5-47fd-e4aa-bbd07c9b85d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86553, 11)\n"
     ]
    }
   ],
   "source": [
    "# Removing purely numeric values from the \"Headline\" column.\n",
    "df = df[~df['Headline'].str.isnumeric()]\n",
    "\n",
    "# Pattern to match the specific numeric formats in headlines\n",
    "pattern = r'\\d+\\.[A-Z]{2,3}&gt;|\\b\\d+\\.\\d+\\b|\\d+am|\\d+pm|\\d+s|\\d{2,}'\n",
    "\n",
    "\n",
    "# Using str.replace to remove the matched patterns with an empty string, but only modify the 'Headline' column\n",
    "df['Headline'] = df['Headline'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Save the filtered DataFrame to a CSV file\n",
    "df.to_csv('no_numeric_headlines.csv', index=False)\n",
    "\n",
    "# Display the shape of the DataFrame to confirm no columns were removed\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2d7ca681",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d7ca681",
    "outputId": "f15528e4-a787-4b15-fd36-1cba7db2cf94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Headline  \\\n",
      "0      Obama Lays Wreath at Arlington National Cemete...   \n",
      "1      Tim Haywood, investment director business-unit...   \n",
      "2      Nouriel Roubini, NYU professor and chairman at...   \n",
      "3      Finland's economy expanded marginally in the t...   \n",
      "4      Tourism and public spending continued to boost...   \n",
      "...                                                  ...   \n",
      "93234  The June employment report is viewed as a cruc...   \n",
      "93235  In addition, establish stimulating economic po...   \n",
      "93236  The Palestinian government spends nearly $ mil...   \n",
      "93237  Palestine Youth Orchestra prepares for first U...   \n",
      "93238  Goldstein, the proprietor of the TG Travel Gro...   \n",
      "\n",
      "                                          Clean_Headline  \n",
      "0      Obama Lays Wreath at Arlington National Cemete...  \n",
      "1      Tim Haywood, investment director business-unit...  \n",
      "2      Nouriel Roubini, NYU professor and chairman at...  \n",
      "3      Finland's economy expanded marginally in the t...  \n",
      "4      Tourism and public spending continued to boost...  \n",
      "...                                                  ...  \n",
      "93234  The June employment report is viewed as a cruc...  \n",
      "93235  In addition, establish stimulating economic po...  \n",
      "93236  The Palestinian government spends nearly $ mil...  \n",
      "93237  Palestine Youth Orchestra prepares for first U...  \n",
      "93238  Goldstein, the proprietor of the TG Travel Gro...  \n",
      "\n",
      "[86553 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "# Function to remove accents\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "# Apply the function to the 'Headline' column\n",
    "df['Clean_Headline'] = df['Headline'].apply(remove_accents)\n",
    "\n",
    "# Display the original and cleaned headlines\n",
    "print(df[['Headline', 'Clean_Headline']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "73011f6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73011f6c",
    "outputId": "8427fb42-a386-4692-f49c-1881823d770c"
   },
   "outputs": [],
   "source": [
    "# from spellchecker import SpellChecker\n",
    "\n",
    "# Initialize the spell checker\n",
    "# spell = SpellChecker()\n",
    "\n",
    "# Function to remove unrecognized words\n",
    "# def remove_typos(text):\n",
    "    # Split the text into words\n",
    "    # words = text.split()\n",
    "    # Check each word against the spell checker\n",
    "    # valid_words = [word for word in words if word in spell or len(spell.unknown([word])) == 0]\n",
    "    # Join and return the valid words as a new string\n",
    "    # return ' '.join(valid_words)\n",
    "\n",
    "# Apply the function to the 'Headline' column\n",
    "# df['Cleaned_Headline'] = df['Clean_Headline'].apply(remove_typos)\n",
    "\n",
    "# Display the DataFrame to verify changes\n",
    "# print(df[['Clean_Headline', 'Cleaned_Headline']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5f8784fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f8784fe",
    "outputId": "5f16c684-82cf-4318-b581-e063a445cb38"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "\n",
    "word_list = set(words.words())\n",
    "\n",
    "# def remove_non_dictionary_words(text):\n",
    "#    return ' '.join(word for word in text.split() if word in word_list)\n",
    "\n",
    "# Assuming 'text' is a string containing your text data\n",
    "# df['Cleaned_Headline'] = df['Cleaned_Headline'].apply(remove_non_dictionary_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "92e7c41b-38ca-4180-9aae-7eacf3431a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_alphabet(text):\n",
    "    return ' '.join(word for word in text.split() if word.isalnum)\n",
    "df['Clean_Headline'] = df['Clean_Headline'].apply(filter_alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e15d4b46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e15d4b46",
    "outputId": "ae75c881-fc95-4682-ec79-b84b64b42c6a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ebff5110",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebff5110",
    "outputId": "b94d4025-90c7-4aa8-d6c6-c51a22997952"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [en:0.9999973888774673]\n",
       "1        [en:0.9999971737689218]\n",
       "2        [en:0.9999975941868632]\n",
       "3        [en:0.9999959447915405]\n",
       "4        [en:0.9999972473097253]\n",
       "                  ...           \n",
       "93234    [en:0.9999961876453028]\n",
       "93235    [en:0.9999971760977238]\n",
       "93236    [en:0.9999952596078846]\n",
       "93237    [en:0.9999953428939599]\n",
       "93238    [en:0.9999977211943405]\n",
       "Name: Clean_Headline, Length: 86553, dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from langdetect import detect_langs\n",
    "from langdetect import detect_langs, LangDetectException\n",
    "\n",
    "# Detecting the language of each headline\n",
    "def detect_language_safe(text):\n",
    "    try:\n",
    "        return detect_langs(text)\n",
    "    except LangDetectException:\n",
    "        return ['unknown']\n",
    "\n",
    "sample_headlines = df['Clean_Headline'].astype(str)\n",
    "detected_languages = sample_headlines.apply(lambda x: detect_language_safe(x))\n",
    "detected_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e9f56d56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9f56d56",
    "outputId": "1c0fd1ec-96ba-4119-a165-ebc991173b31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of headlines: 86553\n",
      "Number of headlines with only English Language headlines: 86282\n"
     ]
    }
   ],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        languages = detect_langs(text)\n",
    "        return languages[0].lang\n",
    "    except LangDetectException:\n",
    "        return None\n",
    "\n",
    "# Detect language in batches\n",
    "batch_size = 1000\n",
    "languages = []\n",
    "\n",
    "for i in range(0, df.shape[0], batch_size):\n",
    "    batch = df['Clean_Headline'][i:i+batch_size].astype(str)\n",
    "    batch_languages = batch.apply(detect_language)\n",
    "    languages.extend(batch_languages)\n",
    "\n",
    "# Adding detected languages as a new column to the DataFrame\n",
    "df['Language'] = languages\n",
    "\n",
    "# Filtering out non-English headlines\n",
    "df_filtered = df[df['Language'] == 'en']\n",
    "\n",
    "\n",
    "print(f\"Original number of headlines: {df.shape[0]}\")\n",
    "print(f\"Number of headlines with only English Language headlines: {df_filtered.shape[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9f5baeb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "9f5baeb8",
    "outputId": "e6dca07d-4224-404f-9988-a4431f04aca7"
   },
   "outputs": [],
   "source": [
    "# Save the filtered DataFrame to a CSV file\n",
    "df_filtered.to_csv('filtered_headlines.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3136b7fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3136b7fb",
    "outputId": "0458e27c-a2a9-421f-8144-ace3fef2b666"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "22cc2827",
   "metadata": {
    "id": "22cc2827"
   },
   "outputs": [],
   "source": [
    "#Preprocess the Text Data\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s\\+\\¬\\†]', ' ', text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(token) for token in lemmatized_text]\n",
    "\n",
    "    # Re-join tokens into a string\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9b9a43d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b9a43d7",
    "outputId": "c769b278-233a-4441-b1e8-037bcc3fc64e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Headline  \\\n",
      "0      Obama Lays Wreath at Arlington National Cemete...   \n",
      "1      Tim Haywood, investment director business-unit...   \n",
      "2      Nouriel Roubini, NYU professor and chairman at...   \n",
      "3      Finland's economy expanded marginally in the t...   \n",
      "4      Tourism and public spending continued to boost...   \n",
      "...                                                  ...   \n",
      "93234  The June employment report is viewed as a cruc...   \n",
      "93235  In addition, establish stimulating economic po...   \n",
      "93236  The Palestinian government spends nearly $ mil...   \n",
      "93237  Palestine Youth Orchestra prepares for first U...   \n",
      "93238  Goldstein, the proprietor of the TG Travel Gro...   \n",
      "\n",
      "                                      Processed_Headline  \n",
      "0      obama lay wreath arlington nation cemeteri pre...  \n",
      "1      tim haywood invest director busi unit head fix...  \n",
      "2      nouriel roubini nyu professor chairman roubini...  \n",
      "3      finland economi expand margin three month end ...  \n",
      "4      tourism public spend continu boost economi jan...  \n",
      "...                                                  ...  \n",
      "93234  june employ report view crucial gaug health ec...  \n",
      "93235  addit establish stimul econom polici budget co...  \n",
      "93236  palestinian govern spend near million annual s...  \n",
      "93237  palestin youth orchestra prepar first uk tour ...  \n",
      "93238  goldstein proprietor tg travel group llc custo...  \n",
      "\n",
      "[86282 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/8xzncctn7wvcmd0xlnrxhl580000gn/T/ipykernel_8538/753346087.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['Processed_Headline'] = df_filtered['Clean_Headline'].apply(preprocess_text)\n"
     ]
    }
   ],
   "source": [
    "# Apply the preprocessing function to each headline\n",
    "df_filtered['Processed_Headline'] = df_filtered['Clean_Headline'].apply(preprocess_text)\n",
    "\n",
    "# Display the processed headlines\n",
    "print(df_filtered[['Headline', 'Processed_Headline']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ecff40",
   "metadata": {
    "id": "51ecff40"
   },
   "source": [
    "### Document-Term Matrix (DTM): Transform the preprocessed headlines into a DTM, which quantifies the headlines by the occurrence of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "affbaba9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "affbaba9",
    "outputId": "9e2f5014-2595-4027-e6e0-4a94998f9bc9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kynanami/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "540cb937",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "540cb937",
    "outputId": "85007e62-8415-451b-a67e-8d987fc54a85"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0ft</th>\n",
       "      <th>0n</th>\n",
       "      <th>0qmsoq9qn2</th>\n",
       "      <th>0xb</th>\n",
       "      <th>0ya</th>\n",
       "      <th>1a</th>\n",
       "      <th>1b</th>\n",
       "      <th>1billion</th>\n",
       "      <th>1bn</th>\n",
       "      <th>1d</th>\n",
       "      <th>...</th>\n",
       "      <th>zurbruegg</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zvezda</th>\n",
       "      <th>zvi</th>\n",
       "      <th>zvishavan</th>\n",
       "      <th>zwane</th>\n",
       "      <th>zyba</th>\n",
       "      <th>zynga</th>\n",
       "      <th>zyoud</th>\n",
       "      <th>zz6hhrfgiw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86277</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86278</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86279</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86280</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86281</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86282 rows × 31049 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0ft  0n  0qmsoq9qn2  0xb  0ya  1a  1b  1billion  1bn  1d  ...  \\\n",
       "0        0   0           0    0    0   0   0         0    0   0  ...   \n",
       "1        0   0           0    0    0   0   0         0    0   0  ...   \n",
       "2        0   0           0    0    0   0   0         0    0   0  ...   \n",
       "3        0   0           0    0    0   0   0         0    0   0  ...   \n",
       "4        0   0           0    0    0   0   0         0    0   0  ...   \n",
       "...    ...  ..         ...  ...  ...  ..  ..       ...  ...  ..  ...   \n",
       "86277    0   0           0    0    0   0   0         0    0   0  ...   \n",
       "86278    0   0           0    0    0   0   0         0    0   0  ...   \n",
       "86279    0   0           0    0    0   0   0         0    0   0  ...   \n",
       "86280    0   0           0    0    0   0   0         0    0   0  ...   \n",
       "86281    0   0           0    0    0   0   0         0    0   0  ...   \n",
       "\n",
       "       zurbruegg  zurich  zvezda  zvi  zvishavan  zwane  zyba  zynga  zyoud  \\\n",
       "0              0       0       0    0          0      0     0      0      0   \n",
       "1              0       0       0    0          0      0     0      0      0   \n",
       "2              0       0       0    0          0      0     0      0      0   \n",
       "3              0       0       0    0          0      0     0      0      0   \n",
       "4              0       0       0    0          0      0     0      0      0   \n",
       "...          ...     ...     ...  ...        ...    ...   ...    ...    ...   \n",
       "86277          0       0       0    0          0      0     0      0      0   \n",
       "86278          0       0       0    0          0      0     0      0      0   \n",
       "86279          0       0       0    0          0      0     0      0      0   \n",
       "86280          0       0       0    0          0      0     0      0      0   \n",
       "86281          0       0       0    0          0      0     0      0      0   \n",
       "\n",
       "       zz6hhrfgiw  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "...           ...  \n",
       "86277           0  \n",
       "86278           0  \n",
       "86279           0  \n",
       "86280           0  \n",
       "86281           0  \n",
       "\n",
       "[86282 rows x 31049 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "#\n",
    "dtm = vectorizer.fit_transform(df_filtered['Processed_Headline'])\n",
    "\n",
    "\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "# dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f63517a1-7057-46ff-b198-de7d22479d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=4)\n",
    "ldamodel = lda.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b4d5a8f1-ba74-4163-a1fc-7ded2faba2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "palestin palestinian minist israel isra prime state nation east intern\n",
      "Topic 1:\n",
      "economi econom year growth quot said global china quarter percent\n",
      "Topic 2:\n",
      "microsoft new window compani announc one quot app develop updat\n",
      "Topic 3:\n",
      "obama presid barack quot said washington hous state administr white\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(ldamodel, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a870559b-393d-43fa-a07d-84373ea9844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist = ldamodel.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "360f3df9-d63f-4dc0-a645-8d482f543578",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_topics = [None]*len(topic_dist)\n",
    "for i in range(0, len(topic_dist)-1):\n",
    "    classified_topics[i] = pd.Series(topic_dist[i]).idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ac2b46bf-37fa-495c-beab-b466487d6ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11150 26116 21589 27426\n"
     ]
    }
   ],
   "source": [
    "print(classified_topics.count(0), classified_topics.count(1), classified_topics.count(2), classified_topics.count(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8c4490e0-b0ec-426a-a01a-06629e721424",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist = pd.DataFrame(topic_dist)\n",
    "df_filtered = pd.concat([df_filtered, topic_dist], axis=1)\n",
    "df_filtered.to_csv('filtered_headlines.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3633289",
   "metadata": {
    "id": "d3633289"
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da8c4705",
   "metadata": {
    "id": "da8c4705"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AV3D2LLpuX6R",
   "metadata": {
    "id": "AV3D2LLpuX6R"
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "dtm_sparse = sparse.csr_matrix(dtm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962cc84c",
   "metadata": {
    "id": "962cc84c"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "dtm_scaled = scaler.fit_transform(dtm_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "tDDNBrC7xbTa",
   "metadata": {
    "id": "tDDNBrC7xbTa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "index = np.arange(85919)\n",
    "batches = np.split(np.arange(90000), 9)\n",
    "batches[-1] = np.arange(80000, 85907)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41e5cd76",
   "metadata": {
    "id": "41e5cd76",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Initialize IncrementalPCA with default parameters\n",
    "ipca = IncrementalPCA()\n",
    "\n",
    "# Assuming you have a batches function defined properly\n",
    "for batch in batches:\n",
    "  dtm_batch = dtm_df.iloc[batch]\n",
    "  dtm_batch = preprocessing.scale(dtm_batch)\n",
    "  ipca.partial_fit(dtm_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "imWg6fKr4H2J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imWg6fKr4H2J",
    "outputId": "10117de2-908f-49a8-ff17-bee6cbd501f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9605"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_var = ipca.explained_variance_ratio_.cumsum()\n",
    "len(exp_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c256c2b6-4ca9-4f9b-a1e0-c9cbbd76bcf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exp_var' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mexp_var\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exp_var' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(exp_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b6c7375-f38e-4e1e-ad98-4c784ca6b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "pc_reduced = ipca.components_[:,:np.argmax(exp_var>threshold)]\n",
    "pc_reduced.shape\n",
    "np.savetxt(\"principal_components.csv\", pc_reduced, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103ce71",
   "metadata": {
    "id": "7103ce71"
   },
   "outputs": [],
   "source": [
    "# #Perform PCA directly\n",
    "\n",
    "# pca = PCA().fit(dtm_df)\n",
    "# cumulative_variance_ratio = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# # Find the # of components that account for a 95% cumulative variance\n",
    "# n_components = len(cumulative_variance_ratio[cumulative_variance_ratio <= 0.95]) + 1\n",
    "\n",
    "# n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29881b43",
   "metadata": {
    "id": "29881b43"
   },
   "outputs": [],
   "source": [
    "# # Initialize PCA, choose the number of components\n",
    "# pca = PCA(n_components=#)\n",
    "\n",
    "# # Fit PCA on the DTM or standardized DTM\n",
    "# dtm_pca = pca.fit_transform(dtm_df)\n",
    "\n",
    "# dtm_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b20f3",
   "metadata": {
    "id": "963b20f3"
   },
   "outputs": [],
   "source": [
    "# # Randomized PCA\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=100, svd_solver='randomized')\n",
    "\n",
    "# dtm_pca = pca.fit(dtm_df)\n",
    "\n",
    "# dtm_pca"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
